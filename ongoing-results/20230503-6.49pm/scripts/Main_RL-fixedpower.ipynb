{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque \n",
    "import math\n",
    "import gym\n",
    "from gym import logger,spaces\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from UAV_ENV.envs.DQNenv_fixedpower import UAVenv_fixedpower\n",
    "from DQN_PyTorch_fixedpower import dqn_agent_fixedpower\n",
    "#from DQN import DQNagent\n",
    "import seaborn as sns \n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# registering the created environment in OpenAI gym : https://www.youtube.com/watch?v=kd4RrN-FTWY\n",
    "# creating an environment object\n",
    "#env=DQNUAVenv()\n",
    "env= gym.make('DqnUavEnv-v3')\n",
    "env.seed(0)\n",
    "Observations=env.observation_space.shape[0]\n",
    "#Observations_n=env.observation_space.n\n",
    "Observation_shape=env.observation_space.shape\n",
    "state=env.reset()\n",
    "Actions_shape=env.action_space\n",
    "Actions=env.action_space.n\n",
    "print('Observation_space:',Observations)\n",
    "#print('Observation_number:',Observations_n)\n",
    "print('Observation_shape:',Observation_shape)\n",
    "print('Action_space:',Actions_shape)\n",
    "print('Action_number:', Actions)\n",
    "print('Initial_state: ',state)\n",
    "#env.path_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd0942",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creating DQN agent\n",
    "seed=0\n",
    "Agent_DQN=dqn_agent_fixedpower(Observations,Actions,seed=0)\n",
    "# creating DDPG agent\n",
    "#Agent_DDPG=ddpg_agent(Observation_n,Action_dim)  # create environment object before calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804cee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the dqn agent\n",
    "num_episode=4500\n",
    "step_per_episode=200\n",
    "reward_per_episode=[]\n",
    "Energy_per_episode=[]\n",
    "Data_Rate_per_episode=[]\n",
    "Avg_reward=[]\n",
    "Avg_energy=[]\n",
    "Avg_Rate=[]\n",
    "losses_per_episode=[]\n",
    "exploration_rate=1\n",
    "epsilon_decay=0.995\n",
    "Min_epsilon=0.001\n",
    "\n",
    "# saving all location in a array\n",
    "x_location=[]\n",
    "y_location=[]\n",
    "z_location=[]\n",
    "\n",
    "for i in range (num_episode):\n",
    "    # reset environment\n",
    "    state=env.reset()\n",
    "    #print(\"state is:\" ,state)\n",
    "    cum_reward=0\n",
    "    cum_rate=0\n",
    "    energy=0\n",
    "    loss=0\n",
    "    done=False\n",
    "    #epsilon=Max_epsilon\n",
    "    \n",
    "    for j in range (step_per_episode-1):\n",
    "        #print(\"state is:\" ,state)\n",
    "        \n",
    "        # action selection\n",
    "        action=Agent_DQN.choose_action(state,exploration_rate)\n",
    "        #print(\"Action is: \",action)\n",
    "        \n",
    "        # calculate reward for chosen action\n",
    "        next_state,reward,done,Data_Rate=env.step(action)\n",
    "        #print(reward)\n",
    "        # save transition \n",
    "        Agent_DQN.reply_buffer(state,action,reward,next_state,done)\n",
    "        energy_per_step,sum_energy=env.energy(state,next_state[0:3])\n",
    "        \n",
    "        #print(\"reward,next_state,done,energy: \",reward,next_state,done,energy_per_step)\n",
    "        #print('========================')\n",
    "        \n",
    "        state=next_state\n",
    "        cum_reward+=reward\n",
    "        cum_rate+=Data_Rate[\"Rate\"]\n",
    "        \n",
    "        #energy+=energy_per_step\n",
    "        \n",
    "        if i==num_episode-1:\n",
    "            x_location.append(next_state[0])\n",
    "            y_location.append(next_state[1])\n",
    "            z_location.append(next_state[2])\n",
    "         \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        # sampling for agent training\n",
    "        loss=Agent_DQN.sample_buffer()\n",
    "        if loss is not None:\n",
    "            loss+=loss\n",
    "\n",
    "    exploration_rate *= epsilon_decay\n",
    "    exploration_rate= max(exploration_rate,Min_epsilon)\n",
    "        \n",
    "    # average reward calculation\n",
    "    losses_per_episode.append(loss)\n",
    "    reward_per_episode.append(cum_reward)\n",
    "    Avg_reward.append(np.mean(reward_per_episode))\n",
    "    Energy_per_episode.append(sum_energy)\n",
    "    Avg_energy.append(np.mean(Energy_per_episode))\n",
    "    Data_Rate_per_episode.append(cum_rate)\n",
    "    Avg_Rate.append(np.mean(Data_Rate_per_episode))\n",
    "    \n",
    "    # print the training results\n",
    "    if (i%2==0):\n",
    "        print('\\rEpisode {} \\t Average Reward: {:.2f},\\t Loss: {:.2f}, \\t Energy:{:.2f}'.\n",
    "              format(i, Avg_reward[i],losses_per_episode[i],Energy_per_episode[i]))\n",
    "                                                                                         \n",
    "        #print('\\rEpisode {}\\tloss: {:.2f}'.format(i, losses[i]))\n",
    "\n",
    "    # save parameters of main network \n",
    "    if i==num_episode-1:\n",
    "        # create a file to save weights and bias\n",
    "        FILE=\"model_parameters.pth\"\n",
    "        torch.save(Agent_DQN.main_network.state_dict(),FILE)\n",
    "\n",
    "# reward visualization\n",
    "fig1 = plt.figure()\n",
    "sns.set()\n",
    "plt.plot(Avg_reward)\n",
    "plt.xlabel('Number of Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "\n",
    "norm=[]\n",
    "for k in Avg_reward:\n",
    "    k=(k-min(Avg_reward))/(max(Avg_reward)-min(Avg_reward))\n",
    "    norm.append(k)\n",
    "fig2 = plt.figure()\n",
    "sns.set()\n",
    "plt.plot(norm)\n",
    "plt.xlabel('Number of Episode')\n",
    "plt.ylabel('Normalized Average Reward')\n",
    "\n",
    "# loss visualization\n",
    "# convert loss values which are tensor values to a list of numpy array\n",
    "losses_per_episode=[k.detach().numpy() for k in losses_per_episode if k is not None]\n",
    "fig3=plt.figure()\n",
    "sns.set()\n",
    "plt.plot(losses_per_episode)\n",
    "plt.xlabel('Number of Episode')\n",
    "plt.ylabel('Loss Function')\n",
    "\n",
    "# energy usage visualization\n",
    "fig4 = plt.figure()\n",
    "sns.set()\n",
    "plt.plot(Avg_energy)\n",
    "plt.xlabel('Number of Episode')\n",
    "plt.ylabel('Average Energy Usage (kJ)')\n",
    "\n",
    "fig5 = plt.figure()\n",
    "sns.set()\n",
    "plt.plot(Energy_per_episode)\n",
    "plt.xlabel('Number of Episode')\n",
    "plt.ylabel('Energy Usage (kJ) ')\n",
    "\n",
    "# energy usage visualization\n",
    "fig6 = plt.figure()\n",
    "sns.set()\n",
    "plt.plot(Avg_Rate)\n",
    "plt.xlabel('Number of Episode')\n",
    "plt.ylabel('Average Data Rate (kb/s)')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbcb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uav trajectory for last episode\n",
    "# the reason why uav is jumped to the terminal state is that I appened the destination to the 3 arrays, fix it!\n",
    "#print(x_location)\n",
    "#print(y_location)\n",
    "#print(z_location)\n",
    "sns.set_style(\"whitegrid\")\n",
    "env.path_plot(x_location,y_location,z_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfe9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results for plotting: lr=0.003\n",
    "from numpy import savetxt\n",
    "savetxt('Average_reward_fixed_power.csv', Avg_reward, delimiter=',')\n",
    "savetxt('reward_per_episode_fixed_power.csv', reward_per_episode, delimiter=',')\n",
    "savetxt('Normalized_Average_reward_fixedpower.csv', norm, delimiter=',')\n",
    "savetxt('losses_per_episode_fixed_power.csv', losses_per_episode, delimiter=',')\n",
    "savetxt('Energy_per_episode_fixed_power.csv', Energy_per_episode, delimiter=',')\n",
    "savetxt('Average_energy_fixed_power.csv', Avg_energy, delimiter=',')\n",
    "savetxt('Avg_Rate_fixed_power.csv', Avg_Rate, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parameters of saved dqn\n",
    "Agent_DQN.main_network.load_state_dict(torch.load(FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b95bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa857f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
